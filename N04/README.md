[![GitHub watchers](https://img.shields.io/badge/tulip--lab-Open--Projects-brightgreen)](../README.md)
[![GitHub watchers](https://img.shields.io/badge/Module-NEXUS-orange)](README.md)

# `NEXUS` Security of Deep Learning Models

In recent years, artificial neural networks, especially deep learning models, have garnered widespread adoption across diverse domains, including engineering, management, biomedicine, and beyond. Their remarkable success has sparked growing concerns regarding their security and reliability. Extensive studies have unveiled the vulnerability of well-designed artificial neural networks to various forms of adversarial attacks, which can cause them to make incorrect and potentially harmful decisions. This security issue has hindered the deployment of artificial neural networks in security-sensitive applications, underscoring the urgent need for robust defense mechanisms.

In response, significant research efforts have been dedicated to designing appropriate defense strategies against adversarial attacks, with the aim of fortifying the security and reliability of artificial neural networks. Many of these defense mechanisms have demonstrated promising performance in mitigating certain types of attacks. However, the field of artificial neural network security remains an active and crucial area of investigation within the AI domain. This is due to the continuous emergence of novel neural network architectures and the concurrent development of sophisticated adversarial attacks tailored to exploit their vulnerabilities.

The relentless pursuit of secure and reliable artificial neural networks is driven by the imperative to safeguard their deployment in critical applications, where erroneous or malicious outputs could have severe consequences. Researchers are continuously exploring innovative defense techniques, ranging from adversarial training and input preprocessing to model verification and certified robustness. The ultimate goal is to **develop artificial neural networks that are not only accurate and efficient but also resilient to adversarial threats**, ensuring their trustworthiness and enabling their widespread adoption in security-sensitive domains.

### :notebook_with_decorative_cover: `NEXUS-S1`

The related documents are encrypted, and you will receive the password upon the acceptance into stage :one: of the project. 

- [NEXUS-S1 Guideline](https://github.com/tulip-lab/handouts/blob/main/nexus/Nexus-S1.pdf) 
- [Stage 1 - N04: Security of Deep Learning Models](https://github.com/tulip-lab/handouts/blob/main/nexus/N04-S1.pdf) 

##### Outstanding Student Works

- TO BE ADDED

### :notebook_with_decorative_cover: `NEXUS-S2`

The related documents are encrypted, and you will receive the password upon the acceptance into the stage :two: of the project. 

- [NEXUS-S2 Guideline](https://github.com/tulip-lab/handouts/blob/main/nexus/Nexus-S2.pdf) 
- [Stage 2 - N04: Security of Deep Learning Models](https://github.com/tulip-lab/handouts/blob/main/nexus/N04-S2.pdf) 

##### Outstanding Student Works

- TO BE ADDED


### :notebook_with_decorative_cover: `NEXUS-S3`


The related documents are encrypted, and you will receive the password upon the acceptance into the stage :three: of the project. 

- [NEXUS-S3 Guideline](https://github.com/tulip-lab/handouts/blob/main/nexus/Nexus-S3.pdf) 
- [Stage 3 - N04: Security of Deep Learning Models](https://github.com/tulip-lab/handouts/blob/main/nexus/N04-S3.pdf) 

##### Outstanding Student Works

- TO BE ADDED
